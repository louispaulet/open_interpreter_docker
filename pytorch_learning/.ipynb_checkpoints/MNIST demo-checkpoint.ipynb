{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab8603a-9ae0-4dba-929d-98957522fe73",
   "metadata": {},
   "source": [
    "# MNIST Pytorch demo\n",
    "\n",
    "We load the MNIST dataset and train a model on it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5640af17-5385-4738-88a0-76ed196acf51",
   "metadata": {},
   "source": [
    "## CPU version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dab7643-cd58-473a-86ea-40f2d5d4d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "840187d5-091c-453f-b003-cf21a13ae81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe93567-1e41-4bd7-84c3-3257043d36fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations for the training data and testing data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b370073-bdf3-4119-982a-8a2c2d3f80d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:12<00:00, 807700.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 367641.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:02<00:00, 811494.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 3453685.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and load the training data\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc06c01-6dce-4a31-b9c8-e69a5e6cd123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the test data\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7e25b7-6be2-4f32-93ea-cc1e4db03301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the network\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5918655e-45cd-4876-9b44-9873b4ee8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "282735d3-ddac-4196-ba82-d4f83e349793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1, 100] loss: 1.138\n",
      "Epoch [1, 200] loss: 0.447\n",
      "Epoch [1, 300] loss: 0.397\n",
      "Epoch [1, 400] loss: 0.352\n",
      "Epoch [1, 500] loss: 0.338\n",
      "Epoch [1, 600] loss: 0.301\n",
      "Epoch [1, 700] loss: 0.295\n",
      "Epoch [1, 800] loss: 0.268\n",
      "Epoch [1, 900] loss: 0.282\n",
      "Epoch [2, 100] loss: 0.243\n",
      "Epoch [2, 200] loss: 0.234\n",
      "Epoch [2, 300] loss: 0.201\n",
      "Epoch [2, 400] loss: 0.219\n",
      "Epoch [2, 500] loss: 0.202\n",
      "Epoch [2, 600] loss: 0.197\n",
      "Epoch [2, 700] loss: 0.188\n",
      "Epoch [2, 800] loss: 0.177\n",
      "Epoch [2, 900] loss: 0.173\n",
      "Epoch [3, 100] loss: 0.160\n",
      "Epoch [3, 200] loss: 0.167\n",
      "Epoch [3, 300] loss: 0.162\n",
      "Epoch [3, 400] loss: 0.146\n",
      "Epoch [3, 500] loss: 0.155\n",
      "Epoch [3, 600] loss: 0.138\n",
      "Epoch [3, 700] loss: 0.136\n",
      "Epoch [3, 800] loss: 0.119\n",
      "Epoch [3, 900] loss: 0.133\n",
      "Epoch [4, 100] loss: 0.115\n",
      "Epoch [4, 200] loss: 0.116\n",
      "Epoch [4, 300] loss: 0.130\n",
      "Epoch [4, 400] loss: 0.126\n",
      "Epoch [4, 500] loss: 0.122\n",
      "Epoch [4, 600] loss: 0.112\n",
      "Epoch [4, 700] loss: 0.125\n",
      "Epoch [4, 800] loss: 0.101\n",
      "Epoch [4, 900] loss: 0.122\n",
      "Epoch [5, 100] loss: 0.100\n",
      "Epoch [5, 200] loss: 0.110\n",
      "Epoch [5, 300] loss: 0.103\n",
      "Epoch [5, 400] loss: 0.096\n",
      "Epoch [5, 500] loss: 0.094\n",
      "Epoch [5, 600] loss: 0.113\n",
      "Epoch [5, 700] loss: 0.096\n",
      "Epoch [5, 800] loss: 0.109\n",
      "Epoch [5, 900] loss: 0.094\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad20badd-c08d-43a4-adc2-4c6ab75a173c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 96.09%\n"
     ]
    }
   ],
   "source": [
    "# Test the network on the test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e78562-4ba2-4028-ae66-47ed7188f693",
   "metadata": {},
   "source": [
    "## GPU version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9e85021-ccf6-494c-8d5b-f7195da94fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1, 100] loss: 1.015\n",
      "Epoch [1, 200] loss: 0.472\n",
      "Epoch [1, 300] loss: 0.402\n",
      "Epoch [1, 400] loss: 0.325\n",
      "Epoch [1, 500] loss: 0.307\n",
      "Epoch [1, 600] loss: 0.300\n",
      "Epoch [1, 700] loss: 0.280\n",
      "Epoch [1, 800] loss: 0.269\n",
      "Epoch [1, 900] loss: 0.249\n",
      "Epoch [2, 100] loss: 0.221\n",
      "Epoch [2, 200] loss: 0.217\n",
      "Epoch [2, 300] loss: 0.190\n",
      "Epoch [2, 400] loss: 0.193\n",
      "Epoch [2, 500] loss: 0.188\n",
      "Epoch [2, 600] loss: 0.201\n",
      "Epoch [2, 700] loss: 0.182\n",
      "Epoch [2, 800] loss: 0.183\n",
      "Epoch [2, 900] loss: 0.163\n",
      "Epoch [3, 100] loss: 0.153\n",
      "Epoch [3, 200] loss: 0.146\n",
      "Epoch [3, 300] loss: 0.142\n",
      "Epoch [3, 400] loss: 0.151\n",
      "Epoch [3, 500] loss: 0.134\n",
      "Epoch [3, 600] loss: 0.134\n",
      "Epoch [3, 700] loss: 0.129\n",
      "Epoch [3, 800] loss: 0.133\n",
      "Epoch [3, 900] loss: 0.142\n",
      "Epoch [4, 100] loss: 0.109\n",
      "Epoch [4, 200] loss: 0.117\n",
      "Epoch [4, 300] loss: 0.118\n",
      "Epoch [4, 400] loss: 0.106\n",
      "Epoch [4, 500] loss: 0.128\n",
      "Epoch [4, 600] loss: 0.112\n",
      "Epoch [4, 700] loss: 0.109\n",
      "Epoch [4, 800] loss: 0.100\n",
      "Epoch [4, 900] loss: 0.120\n",
      "Epoch [5, 100] loss: 0.095\n",
      "Epoch [5, 200] loss: 0.103\n",
      "Epoch [5, 300] loss: 0.101\n",
      "Epoch [5, 400] loss: 0.086\n",
      "Epoch [5, 500] loss: 0.095\n",
      "Epoch [5, 600] loss: 0.104\n",
      "Epoch [5, 700] loss: 0.089\n",
      "Epoch [5, 800] loss: 0.096\n",
      "Epoch [5, 900] loss: 0.094\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 97.04%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Transformations for the training data and testing data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the network and move it to the GPU\n",
    "net = Net().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        # Move data to the GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Test the network on the test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # Move data to the GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e921c-9ff1-4614-9b88-4f8b72ffdda6",
   "metadata": {},
   "source": [
    "## CNN instead of full Linear  \n",
    "\n",
    "We use convolution layers instead of linear ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6fb2dbf-5a9a-45f1-856a-5abe2e4e8b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1, 100] loss: 0.716\n",
      "Epoch [1, 200] loss: 0.174\n",
      "Epoch [1, 300] loss: 0.130\n",
      "Epoch [1, 400] loss: 0.094\n",
      "Epoch [1, 500] loss: 0.089\n",
      "Epoch [1, 600] loss: 0.073\n",
      "Epoch [1, 700] loss: 0.079\n",
      "Epoch [1, 800] loss: 0.074\n",
      "Epoch [1, 900] loss: 0.061\n",
      "Epoch [2, 100] loss: 0.055\n",
      "Epoch [2, 200] loss: 0.043\n",
      "Epoch [2, 300] loss: 0.057\n",
      "Epoch [2, 400] loss: 0.047\n",
      "Epoch [2, 500] loss: 0.044\n",
      "Epoch [2, 600] loss: 0.040\n",
      "Epoch [2, 700] loss: 0.045\n",
      "Epoch [2, 800] loss: 0.047\n",
      "Epoch [2, 900] loss: 0.047\n",
      "Epoch [3, 100] loss: 0.034\n",
      "Epoch [3, 200] loss: 0.031\n",
      "Epoch [3, 300] loss: 0.029\n",
      "Epoch [3, 400] loss: 0.031\n",
      "Epoch [3, 500] loss: 0.039\n",
      "Epoch [3, 600] loss: 0.030\n",
      "Epoch [3, 700] loss: 0.028\n",
      "Epoch [3, 800] loss: 0.035\n",
      "Epoch [3, 900] loss: 0.035\n",
      "Epoch [4, 100] loss: 0.018\n",
      "Epoch [4, 200] loss: 0.027\n",
      "Epoch [4, 300] loss: 0.016\n",
      "Epoch [4, 400] loss: 0.023\n",
      "Epoch [4, 500] loss: 0.024\n",
      "Epoch [4, 600] loss: 0.020\n",
      "Epoch [4, 700] loss: 0.028\n",
      "Epoch [4, 800] loss: 0.026\n",
      "Epoch [4, 900] loss: 0.024\n",
      "Epoch [5, 100] loss: 0.023\n",
      "Epoch [5, 200] loss: 0.020\n",
      "Epoch [5, 300] loss: 0.015\n",
      "Epoch [5, 400] loss: 0.016\n",
      "Epoch [5, 500] loss: 0.017\n",
      "Epoch [5, 600] loss: 0.018\n",
      "Epoch [5, 700] loss: 0.017\n",
      "Epoch [5, 800] loss: 0.027\n",
      "Epoch [5, 900] loss: 0.013\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 99.18%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Transformations for the training data and testing data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the network and move it to the GPU\n",
    "net = CNN().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        # Move data to the GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Test the network on the test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # Move data to the GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc18396-5bfe-4280-bbb0-50fb63229d27",
   "metadata": {},
   "source": [
    "## Test the saving and loading of a trained model  \n",
    "\n",
    "We save both the model architecture and the weights, then test accuracy to check results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1746baf2-ddb4-424e-8111-edd43dd9c957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire model saved to cnn_mnist_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model\n",
    "model_path = 'cnn_mnist_model.pth'\n",
    "torch.save(net, model_path)\n",
    "print(f'Entire model saved to {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eccbfd7-677c-4d21-ba9c-ff217fde8b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Load the entire model\n",
    "net = torch.load(model_path)\n",
    "net = net.to(device)  # Ensure the model is moved to the correct device\n",
    "net.eval()  # Set the model to evaluation mode\n",
    "print('Model loaded and ready for evaluation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "394ffdf3-f1ff-42a7-bec8-cd8c96be4dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99.18%\n"
     ]
    }
   ],
   "source": [
    "for data in testloader:\n",
    "        images, labels = data\n",
    "        # Move data to the GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15933ece-a3c5-4bd0-ad97-b528797af938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
